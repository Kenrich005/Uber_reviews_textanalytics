{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Understanding the Business Problem\n",
    "Uber Inc in the US wants to know:\n",
    "\n",
    "- the major complaints premium users have about their cab services,\n",
    "- how these impact service ratings.\n",
    "\n",
    "We as (technical) consultants to Uber. have to:  \n",
    "- [a] analyze text reviews of Uber cabs’ US services,  \n",
    "- [b] relate whether and which different features of these reviews impact overall ratings  \n",
    "- [c] pinpoint possible areas of improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pre-processing: \n",
    "- Examine the dataset. \n",
    "- ID the columns of interest. \n",
    "- Drop special characters, html junk etc. \n",
    "- Perform any other preprocessing and text-cleaning activity you think fits this context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author_Name</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author_URL</th>\n",
       "      <th>App_Version</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#NEVERUBER</td>\n",
       "      <td>Dishonest and Disgusting</td>\n",
       "      <td>https://itunes.apple.com/us/reviews/id663331949</td>\n",
       "      <td>3.434.10005</td>\n",
       "      <td>1</td>\n",
       "      <td>For half an hour, we tried EVERY UBER SERVICE ...</td>\n",
       "      <td>29-12-2020 01:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$$Heaven</td>\n",
       "      <td>Free offer</td>\n",
       "      <td>https://itunes.apple.com/us/reviews/id810421958</td>\n",
       "      <td>3.434.10005</td>\n",
       "      <td>2</td>\n",
       "      <td>If I’m not eligible for the offer Stop floodin...</td>\n",
       "      <td>01-01-2021 23:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.Disappointed....</td>\n",
       "      <td>Inaccurate</td>\n",
       "      <td>https://itunes.apple.com/us/reviews/id49598333</td>\n",
       "      <td>3.439.10000</td>\n",
       "      <td>2</td>\n",
       "      <td>Consistently inaccurate Uber Eats ETA and the ...</td>\n",
       "      <td>15-01-2021 23:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.i. andrea</td>\n",
       "      <td>bad</td>\n",
       "      <td>https://itunes.apple.com/us/reviews/id689880334</td>\n",
       "      <td>3.434.10005</td>\n",
       "      <td>1</td>\n",
       "      <td>i had my rides canceled back to back. they the...</td>\n",
       "      <td>08-12-2020 01:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-:deka:-</td>\n",
       "      <td>Double charged me for an order</td>\n",
       "      <td>https://itunes.apple.com/us/reviews/id124963835</td>\n",
       "      <td>3.434.10005</td>\n",
       "      <td>1</td>\n",
       "      <td>Two of the same orders was added by accident. ...</td>\n",
       "      <td>15-12-2020 04:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Author_Name                           Title  \\\n",
       "0         #NEVERUBER        Dishonest and Disgusting   \n",
       "1           $$Heaven                      Free offer   \n",
       "2  .Disappointed....                      Inaccurate   \n",
       "3         .i. andrea                             bad   \n",
       "4           -:deka:-  Double charged me for an order   \n",
       "\n",
       "                                        Author_URL  App_Version  Rating  \\\n",
       "0  https://itunes.apple.com/us/reviews/id663331949  3.434.10005       1   \n",
       "1  https://itunes.apple.com/us/reviews/id810421958  3.434.10005       2   \n",
       "2   https://itunes.apple.com/us/reviews/id49598333  3.439.10000       2   \n",
       "3  https://itunes.apple.com/us/reviews/id689880334  3.434.10005       1   \n",
       "4  https://itunes.apple.com/us/reviews/id124963835  3.434.10005       1   \n",
       "\n",
       "                                              Review              Date  \n",
       "0  For half an hour, we tried EVERY UBER SERVICE ...  29-12-2020 01:14  \n",
       "1  If I’m not eligible for the offer Stop floodin...  01-01-2021 23:17  \n",
       "2  Consistently inaccurate Uber Eats ETA and the ...  15-01-2021 23:38  \n",
       "3  i had my rides canceled back to back. they the...  08-12-2020 01:01  \n",
       "4  Two of the same orders was added by accident. ...  15-12-2020 04:02  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"G:\\ISB AMPBA\\9. Introduction to Text Analytics\\Assignment\\uber_reviews_itune.csv\",\n",
    "                 encoding='cp1252')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns of interest:  \n",
    "1. Title - Brief summary about the review\n",
    "2. Rating - Label for supervised learning\n",
    "3. Review - To extract the sentiment of the complaint\n",
    "4. Date - Extracting weekday or weekend may give better insight on nature of review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dishonest and Disgusting</td>\n",
       "      <td>1</td>\n",
       "      <td>For half an hour, we tried EVERY UBER SERVICE ...</td>\n",
       "      <td>29-12-2020 01:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Free offer</td>\n",
       "      <td>2</td>\n",
       "      <td>If I’m not eligible for the offer Stop floodin...</td>\n",
       "      <td>01-01-2021 23:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Inaccurate</td>\n",
       "      <td>2</td>\n",
       "      <td>Consistently inaccurate Uber Eats ETA and the ...</td>\n",
       "      <td>15-01-2021 23:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bad</td>\n",
       "      <td>1</td>\n",
       "      <td>i had my rides canceled back to back. they the...</td>\n",
       "      <td>08-12-2020 01:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Double charged me for an order</td>\n",
       "      <td>1</td>\n",
       "      <td>Two of the same orders was added by accident. ...</td>\n",
       "      <td>15-12-2020 04:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Title  Rating  \\\n",
       "0        Dishonest and Disgusting       1   \n",
       "1                      Free offer       2   \n",
       "2                      Inaccurate       2   \n",
       "3                             bad       1   \n",
       "4  Double charged me for an order       1   \n",
       "\n",
       "                                              Review              Date  \n",
       "0  For half an hour, we tried EVERY UBER SERVICE ...  29-12-2020 01:14  \n",
       "1  If I’m not eligible for the offer Stop floodin...  01-01-2021 23:17  \n",
       "2  Consistently inaccurate Uber Eats ETA and the ...  15-01-2021 23:38  \n",
       "3  i had my rides canceled back to back. they the...  08-12-2020 01:01  \n",
       "4  Two of the same orders was added by accident. ...  15-12-2020 04:02  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df.drop(['Author_Name','Author_URL','App_Version'],axis=1)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_replace=['<U+0001F621>','<U+0001F615>','<U+0001F44E>']\n",
    "replace_with=['pouting face','confused face','thumbs down']\n",
    "df1.Review=df1.Review.replace(to_replace, replace_with, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(490, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.Review = df1.Review.str.split('<').str[0]\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop empty rows or docs\n",
    "df1.Review[149]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(489, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['Review'].replace('', np.nan, inplace=True)\n",
    "df1.dropna(subset=['Review'], inplace=True)\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and Stemming\n",
    "\n",
    "Below we import NLTK's sentence and word tokenizer, and stemmer. Note the use of list comprehension to bundle both into one  line of efficient code.\n",
    "\n",
    "Note also the use of regex from *re* to detect and drop any non alphabetic characters from the corpus.\n",
    "\n",
    "Find below two straightforward user defined funcs to tokenize (and stem).\n",
    "\n",
    "We will apply these funcs on each doc in the corpus subsequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "import nltk, re, requests\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "## here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation) using regex\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.343\n"
     ]
    }
   ],
   "source": [
    "# Use above funcs to iterate over the list of synopses to create two vocabularies: one stemmed and one only tokenized. \n",
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "reviews = df1.Review.tolist()\n",
    "\n",
    "t0 = time.time()\n",
    "for i in reviews:\n",
    "    \n",
    "    # doing both toknz & stemming\n",
    "    allwords_stemmed = tokenize_and_stem(i)\n",
    "    totalvocab_stemmed.extend(allwords_stemmed)\n",
    "    \n",
    "    # doing toknz only\n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)\n",
    "    \n",
    "t1 = time.time()\n",
    "print(round(t1-t0, 3))    # 0.2 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>half</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>an</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>we</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30606</th>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30607</th>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30608</th>\n",
       "      <td>done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30609</th>\n",
       "      <td>using</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30610</th>\n",
       "      <td>uber</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30611 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       words\n",
       "0        for\n",
       "1       half\n",
       "2         an\n",
       "3       hour\n",
       "4         we\n",
       "...      ...\n",
       "30606      i\n",
       "30607      m\n",
       "30608   done\n",
       "30609  using\n",
       "30610   uber\n",
       "\n",
       "[30611 rows x 1 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create a pandas DataFrame with the stemmed vocabulary as the index and the tokenized words as the column\n",
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized})\n",
    "vocab_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KennyD\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.91 s\n",
      "(489, 23674)\n"
     ]
    }
   ],
   "source": [
    "## Tf-idf and document similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# defining parms for the tfidf-tokenizer here\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=1, # max proportion of docs word is present in\n",
    "                                   max_features=200000,\n",
    "                                   min_df=0, \n",
    "                                   stop_words='english',\n",
    "                                   use_idf=True, \n",
    "                                   tokenizer=tokenize_and_stem, \n",
    "                                   ngram_range=(1,3))\n",
    "\n",
    "# note magic cmd %time\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(reviews)    # 6.05 secs\n",
    "\n",
    "print(tfidf_matrix.shape)    # dimns of the tfidf matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'d\",\n",
       " \"'d enrol\",\n",
       " \"'d enrol reoccur\",\n",
       " \"'m illinoi\",\n",
       " \"'m illinoi inland\",\n",
       " \"'m late\",\n",
       " \"'m late person\",\n",
       " \"'m pretti\",\n",
       " \"'m pretti late\",\n",
       " \"'m rich\",\n",
       " \"'m rich use\",\n",
       " \"'m sick\",\n",
       " \"'m sick fake\",\n",
       " \"'m talk\",\n",
       " \"'m talk just\",\n",
       " \"'m told\",\n",
       " \"'m told email\",\n",
       " \"'s lyft\",\n",
       " \"'s lyft larg\",\n",
       " \"'s minut\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "terms[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(tfidf_matrix))\n",
    "\n",
    "tfidf_matrix.todense()[:5,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(489, 23674)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix.todense()[:10,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
